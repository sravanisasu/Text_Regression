{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4979221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Required imports\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import os as os\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer,BertConfig\n",
    "from transformers import TFBertModel\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.metrics import MeanSquaredError\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370fc969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "###### FinBERT Layer\n",
    "config = BertConfig.from_pretrained(vocab_path='10k-sample/FinVocab-Uncased.txt',pretrained_model_name_or_path='10k-sample//config.json')\n",
    "FinBERT_model = TFBertModel.from_pretrained(config=config,pretrained_model_name_or_path='10k-sample/pytorch_model.bin',from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42811b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to extract the input text from the files ########\n",
    "def process_inp_doc(path_file) :\n",
    "\n",
    "  file_text = open(path_file,encoding='utf8').read()\n",
    "\n",
    "  # remove punctations and digits and remove <PAGE> which was used for page number\n",
    "  file_data = re.sub(r'[\\d$%-:;!]', '', file_text)\n",
    "  file_data = re.sub(r'<PAGE>', '', file_data)\n",
    "  file_data = ''.join(file_data)\n",
    "\n",
    "  return file_data\n",
    "\n",
    "######## Function to extract the output values from the file ########\n",
    "def process_out(company_id,output_file):\n",
    "  \n",
    "  with open(output_file,'r', encoding='utf-8') as m_file :\n",
    "    for line in m_file.readlines():\n",
    "      if company_id == line.split()[1]:\n",
    "        return line.split()[0]\n",
    "    print(\"not found\")\n",
    "  return None\n",
    "\n",
    "######## Function to pre-process the documents from meta-file of a given year ########\n",
    "def pre_processing(meta_file,output_file):\n",
    "  \n",
    "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
    "    \n",
    "    year = meta_file.split('/')[2].split('.')[0]\n",
    "    dir_path = os.path.dirname(meta_file).split('/')[0] + '/all.tok/' +year+'.tok'\n",
    "    data =[]\n",
    "    \n",
    "    for line in m_file.readlines():\n",
    "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
    "\n",
    "      # get input sentences from the company document\n",
    "      inp_sentences = process_inp_doc(inp_path_file)\n",
    "    \n",
    "      # get output value for the company\n",
    "      out_values = float(process_out(line.split()[0],output_file))\n",
    "\n",
    "      #insert values into the data list\n",
    "      data.append({'text':inp_sentences,'value':out_values})\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db053c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to get the encoded values ######## \n",
    "def FinBERT_encode(sentences, tokenizer, MAX_SEQ_LEN=512):\n",
    "\n",
    "  all_tokens = []\n",
    "  all_masks = []\n",
    "  all_segments = []\n",
    "  for sentence in sentences:\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = stokens[-MAX_SEQ_LEN+2:]\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(stokens,)\n",
    "\n",
    "    ids = token_ids + [0] * (MAX_SEQ_LEN-len(token_ids))\n",
    "    masks = [1]*len(token_ids) + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    segments = [0] * (MAX_SEQ_LEN)\n",
    "\n",
    "    all_tokens.append(ids)\n",
    "    all_masks.append(masks)\n",
    "    all_segments.append(segments)\n",
    "\n",
    "  return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d4b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### function that defines the model\n",
    "def get_model():\n",
    "\n",
    "  input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_word_ids\")\n",
    "  input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_mask\")\n",
    "  segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"segment_ids\")\n",
    "\n",
    "  custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
    "\n",
    "  model_output = FinBERT_model(input_word_ids, input_mask, segment_ids)\n",
    "  clf_output = model_output.last_hidden_state\n",
    "      \n",
    "  net = tf.keras.layers.GlobalMaxPool1D()(clf_output)\n",
    "  net = tf.keras.layers.Dense(1, activation='linear')(net)\n",
    "  out = tf.keras.layers.Dense(1, activation='linear', name='output')(net)\n",
    "\n",
    "  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "\n",
    "  opt = optimizers.Adam(learning_rate=0.05)\n",
    "  model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1d919fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000049DB953C40>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000049DB953C40>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_1 (TFBertModel)   TFBaseModelOutputWit 109751808   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 768)          0           tf_bert_model_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            769         global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            2           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 109,752,579\n",
      "Trainable params: 109,752,579\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#### model summary \n",
    "MAX_SEQ_LEN = 512\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49208b35",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '10k-sample/all.meta/2005.meta.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d30dff599407>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m######## extracting text and storing it in dataframes ########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#     with tf.device('/device:GPU:0'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-73be3d52e816>\u001b[0m in \u001b[0;36mpre_processing\u001b[1;34m(meta_file, output_file)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mm_file\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0myear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '10k-sample/all.meta/2005.meta.txt'"
     ]
    }
   ],
   "source": [
    "##### Data extraction and Fitting the model\n",
    "test_loss_all_years = []\n",
    "train_loss_all_years = []\n",
    "val_loss_all_years = []\n",
    "history_all_years = []\n",
    "data = []\n",
    "n_splits = 5\n",
    "epochs = 5\n",
    "for year in range(2008,2014):\n",
    "    \n",
    "    ######## extracting text and storing it in dataframes ########\n",
    "#     with tf.device('/device:GPU:0'):\n",
    "    data_train = pre_processing('10k-sample/all.meta/'+str(year-3)+'.meta.txt','10k-sample/all.logfama/'+str(year-3)+'.logfama.txt')\n",
    "    data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-2)+'.meta.txt','10k-sample/all.logfama/'+str(year-2)+'.logfama.txt'))\n",
    "    data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-1)+'.meta.txt','10k-sample/all.logfama/'+str(year-1)+'.logfama.txt'))\n",
    "    train_df = pd.DataFrame(data_train,columns=['text','value'])\n",
    "\n",
    "    data_test = pre_processing('10k-sample/all.meta/'+str(year)+'.meta.txt','10k-sample/all.logfama/'+str(year)+'.logfama.txt')\n",
    "    test_df = pd.DataFrame(data_test,columns=['text','value'])\n",
    "    data.append({'year':year,'train_df_length':len(data_train),'test_df_length':len(data_test)})\n",
    "    \n",
    "    ###### removing few documents which are not processed properly####\n",
    "    train_df = train_df.loc[train_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "    test_df = test_df.loc[test_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "    \n",
    "    vocab_path = '10k-sample/FinVocab-Uncased.txt'\n",
    "    ######## extracting tokens from dataframes ########\n",
    "\n",
    "    tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, do_basic_tokenize = True)\n",
    "\n",
    "#     with tf.device('/device:GPU:0'):\n",
    "\n",
    "    #### training \n",
    "    # input encoding\n",
    "    sentences = train_df.text.values\n",
    "    FinBERT_train_input = FinBERT_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "    # output values\n",
    "    FinBERT_train_output = train_df.value.values\n",
    "\n",
    "    #### test\n",
    "    # input encoding\n",
    "    sentences = test_df.text.values\n",
    "    FinBERT_test_input = FinBERT_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "    # output values\n",
    "    FinBERT_test_output = test_df.value.values\n",
    "    \n",
    "    ### applying minmax scalar\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    FinBERT_train_output = np.array(FinBERT_train_output).reshape(len(FinBERT_train_output),1)\n",
    "    FinBERT_test_output = np.array(FinBERT_test_output).reshape(len(FinBERT_test_output),1)\n",
    "    output = np.concatenate((FinBERT_train_output, FinBERT_test_output))\n",
    "    output = scaler.fit_transform(output)\n",
    "    FinBERT_train_output = output[:len(FinBERT_train_input[0])]\n",
    "    FinBERT_test_output = output[-len(FinBERT_test_input[0]):]\n",
    "    \n",
    "    ######## Kfold training and saving checkpoints ########\n",
    "#     with tf.device('/device:GPU:0'):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    history =[]\n",
    "    train_loss=[]\n",
    "    vald_loss=[]\n",
    "    test_loss = []\n",
    "    fold = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(FinBERT_train_input[0]):\n",
    "\n",
    "        checkpoint_filepath = 'Results/FinBERT_results_min/CheckPoints/'+str(year)+'FinBERT_checkpoint'+str(fold)\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                                        filepath=checkpoint_filepath,\n",
    "                                                                        save_weights_only=False,\n",
    "                                                                        monitor='loss',\n",
    "                                                                        mode='min',\n",
    "                                                                        save_best_only=True\n",
    "                                                                    )\n",
    "\n",
    "        train_history = model.fit(\n",
    "                                  [FinBERT_train_input[0][train_index],FinBERT_train_input[1][train_index],FinBERT_train_input[2][train_index]],#input\n",
    "                                  FinBERT_train_output[train_index],#output\n",
    "                                  epochs=epochs, #epochs\n",
    "                                  verbose=1 ,\n",
    "                                  batch_size = batch_size,\n",
    "                                  callbacks=[model_checkpoint_callback]\n",
    "                              )\n",
    "        model_best = get_model()\n",
    "        model_best.load_weights(checkpoint_filepath)\n",
    "\n",
    "        fold+=1\n",
    "        loss_T = model_best.evaluate([FinBERT_train_input[0][train_index],FinBERT_train_input[1][train_index],FinBERT_train_input[2][train_index]]\n",
    "                                           , FinBERT_train_output[train_index], verbose=0)\n",
    "        loss_V = model_best.evaluate([FinBERT_train_input[0][test_index],FinBERT_train_input[1][test_index],FinBERT_train_input[2][test_index]]\n",
    "                                          , FinBERT_train_output[test_index], verbose=0)\n",
    "        loss_test = model_best.evaluate([FinBERT_test_input[0],FinBERT_test_input[1],FinBERT_test_input[2]]\n",
    "                                          , FinBERT_test_output, verbose=0)\n",
    "        \n",
    "        train_loss.append(loss_T)\n",
    "        vald_loss.append(loss_V)\n",
    "        history.append(train_history)\n",
    "        test_loss.append(loss_test)\n",
    "            \n",
    "    test_loss_all_years.append(test_loss)\n",
    "    train_loss_all_years.append(train_loss)\n",
    "    val_loss_all_years.append(vald_loss)\n",
    "    history_all_years.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(data,columns=['year','train_df_length','test_df_length'])\n",
    "stats_df.to_csv('Loss_values/BERT_stats_minmax.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "data=[test_loss]\n",
    "  \n",
    "fig = plt.figure()  \n",
    "# Creating axes instance \n",
    "ax = fig.add_axes([0, 0, 1, 1]) \n",
    "  \n",
    "# Creating plot \n",
    "ax.boxplot(data)\n",
    "years = [year for year in range(2008,2014)]\n",
    "ax.set_xticklabels([year for year in range(2008,2014)]) \n",
    "\n",
    "# naming the y axis \n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title(\"Box plot for Test Loss\")\n",
    "textstr ='Test Loss for FinBERT : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
    "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
    "# show plot \n",
    "plt.savefig('Plots/block_plot_FinBERT_minmax.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae609e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = []\n",
    "train_data = []\n",
    "vald_data = []\n",
    "for year_loss_test,year_loss_train,year_loss_vald,year in zip(test_loss_all_years,train_loss_all_years,val_loss_all_years,years) :\n",
    "    loss_data.append({'year':year,'value':year_loss_test})\n",
    "    train_data.append({'year':year,'value':year_loss_train})\n",
    "    vald_data.append({'year':year,'value':year_loss_vald})\n",
    "    \n",
    "loss_data_test_df = pd.DataFrame(loss_data,columns=['year','value'])\n",
    "loss_data_test_df.to_csv('Loss_values/FinBERT_Loss_test_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_train_df = pd.DataFrame(train_data,columns=['year','value'])\n",
    "loss_data_train_df.to_csv('Loss_values/FinBERT_Loss_train_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_vald_df = pd.DataFrame(vald_data,columns=['year','value'])\n",
    "loss_data_vald_df.to_csv('Loss_values/FinBERT_Loss_vald_minmax.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
