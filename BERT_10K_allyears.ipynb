{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c0ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Required imports\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import os as os\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf530d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using C:\\Users\\catuser\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'.\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1: 140.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1: 280.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1: 417.73MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1, Total size: 423.26MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'.\n"
     ]
    }
   ],
   "source": [
    "###### BERT Layer\n",
    "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ded9f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to extract the input text from the files ########\n",
    "def process_inp_doc(path_file) :\n",
    "\n",
    "  file_text = open(path_file,encoding='utf8').read()\n",
    "\n",
    "  # remove punctations and digits and remove <PAGE> which was used for page number\n",
    "  file_data = re.sub(r'[\\d$%-:;!]', '', file_text)\n",
    "  file_data = re.sub(r'<PAGE>', '', file_data)\n",
    "  file_data = ''.join(file_data)\n",
    "\n",
    "  return file_data\n",
    "\n",
    "######## Function to extract the output values from the file ########\n",
    "def process_out(company_id,output_file):\n",
    "  \n",
    "  with open(output_file,'r', encoding='utf-8') as m_file :\n",
    "    for line in m_file.readlines():\n",
    "      if company_id == line.split()[1]:\n",
    "        return line.split()[0]\n",
    "    print(\"not found\")\n",
    "  return None\n",
    "\n",
    "######## Function to pre-process the documents from meta-file of a given year ########\n",
    "def pre_processing(meta_file,output_file):\n",
    "  \n",
    "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
    "    \n",
    "    year = meta_file.split('/')[2].split('.')[0]\n",
    "    dir_path = os.path.dirname(meta_file).split('/')[0] + '/all.tok/' +year+'.tok'\n",
    "    data =[]\n",
    "    for line in m_file.readlines():\n",
    "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
    "\n",
    "      # get input sentences from the company document\n",
    "      inp_sentences = process_inp_doc(inp_path_file)\n",
    "    \n",
    "      # get output value for the company\n",
    "      out_values = float(process_out(line.split()[0],output_file))\n",
    "\n",
    "      #insert values into the data list\n",
    "      data.append({'text':inp_sentences,'value':out_values})\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec19491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to get the encoded values ######## \n",
    "def bert_encode(sentences, tokenizer, MAX_SEQ_LEN=512):\n",
    "\n",
    "  all_tokens = []\n",
    "  all_masks = []\n",
    "  all_segments = []\n",
    "  for sentence in sentences:\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = stokens[-MAX_SEQ_LEN+2:]\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(stokens,)\n",
    "\n",
    "    ids = token_ids + [0] * (MAX_SEQ_LEN-len(token_ids))\n",
    "    masks = [1]*len(token_ids) + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    segments = [0] * (MAX_SEQ_LEN)\n",
    "\n",
    "    all_tokens.append(ids)\n",
    "    all_masks.append(masks)\n",
    "    all_segments.append(segments)\n",
    "\n",
    "  return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bebb8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### function that defines the model\n",
    "def get_model():\n",
    "\n",
    "  input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_word_ids\")\n",
    "  input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_mask\")\n",
    "  segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"segment_ids\")\n",
    "\n",
    "  custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
    "\n",
    "  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "  clf_output = pooled_output\n",
    "  net = tf.keras.layers.Dense(64, activation=custom_objects['leaky_relu'])(clf_output)\n",
    "  out = tf.keras.layers.Dense(1, activation=custom_objects['leaky_relu'], name='output')(net)\n",
    "\n",
    "  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "\n",
    "  opt = optimizers.Adam(learning_rate=0.1)\n",
    "  model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f00cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           49216       keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            65          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 109,531,522\n",
      "Trainable params: 49,281\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#### model summary \n",
    "MAX_SEQ_LEN = 512\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22be3ab6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '10k-sample/all.meta/2005.meta.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fd4a96b4314e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m######## extracting text and storing it in dataframes ########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#     with tf.device('/device:GPU:0'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-80ec9cdf9c22>\u001b[0m in \u001b[0;36mpre_processing\u001b[1;34m(meta_file, output_file)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mm_file\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0myear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '10k-sample/all.meta/2005.meta.txt'"
     ]
    }
   ],
   "source": [
    "##### Data extraction and Fitting the model\n",
    "test_loss_all_years = []\n",
    "train_loss_all_years = []\n",
    "val_loss_all_years = []\n",
    "history_all_years = []\n",
    "data = []\n",
    "n_splits = 5\n",
    "epochs = 5\n",
    "for year in range(2008,2014):\n",
    "    \n",
    "    ######## extracting text and storing it in dataframes ########\n",
    "#     with tf.device('/device:GPU:0'):\n",
    "    data_train = pre_processing('10k-sample/all.meta/'+str(year-3)+'.meta.txt','10k-sample/all.logfama/'+str(year-3)+'.logfama.txt')\n",
    "    data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-2)+'.meta.txt','10k-sample/all.logfama/'+str(year-2)+'.logfama.txt'))\n",
    "    data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-1)+'.meta.txt','10k-sample/all.logfama/'+str(year-1)+'.logfama.txt'))\n",
    "    train_df = pd.DataFrame(data_train,columns=['text','value'])\n",
    "\n",
    "    data_test = pre_processing('10k-sample/all.meta/'+str(year)+'.meta.txt','10k-sample/all.logfama/'+str(year)+'.logfama.txt')\n",
    "    test_df = pd.DataFrame(data_test,columns=['text','value'])\n",
    "    data.append({'year':year,'train_df_length':len(data_train),'test_df_length':len(data_test)})\n",
    "    \n",
    "    ###### removing few documents which are not processed properly####\n",
    "    train_df = train_df.loc[train_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "    test_df = test_df.loc[test_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "    \n",
    "    ######## extracting tokens from dataframes ########\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # with tf.device('/device:GPU:0'):\n",
    "\n",
    "    #### training \n",
    "    # input encoding\n",
    "    sentences = train_df.text.values\n",
    "    bert_train_input = bert_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "    # output values\n",
    "    bert_train_output = train_df.value.values\n",
    "\n",
    "    #### test\n",
    "    # input encoding\n",
    "    sentences = test_df.text.values\n",
    "    bert_test_input = bert_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "    # output values\n",
    "    bert_test_output = test_df.value.values\n",
    "    \n",
    "    ### applying minmax scalar\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    bert_train_output = np.array(bert_train_output).reshape(len(bert_train_output),1)\n",
    "    bert_test_output = np.array(bert_test_output).reshape(len(bert_test_output),1)\n",
    "    output = np.concatenate((bert_train_output, bert_test_output))\n",
    "    output = scaler.fit_transform(output)\n",
    "    bert_train_output = output[:len(bert_train_input[0])]\n",
    "    bert_test_output = output[-len(bert_test_input[0]):]    \n",
    "\n",
    "    ######## Kfold training and saving checkpoints ########\n",
    "#     with tf.device('/device:GPU:0'):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    history =[]\n",
    "    train_loss=[]\n",
    "    vald_loss=[]\n",
    "    test_loss = []\n",
    "    fold = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(bert_train_input[0]):\n",
    "\n",
    "        checkpoint_filepath = 'Results/BERT_results_min/CheckPoints/'+str(year)+'BERT_checkpoint'+str(fold)\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                                        filepath=checkpoint_filepath,\n",
    "                                                                        save_weights_only=False,\n",
    "                                                                        monitor='loss',\n",
    "                                                                        mode='min',\n",
    "                                                                        save_best_only=True\n",
    "                                                                        )\n",
    "\n",
    "        train_history = model.fit(\n",
    "                                  [bert_train_input[0][train_index],bert_train_input[1][train_index],bert_train_input[2][train_index]],#input\n",
    "                                  bert_train_output[train_index],#output\n",
    "                                  epochs=epochs, #epochs\n",
    "                                  verbose=1,\n",
    "                                  callbacks=[model_checkpoint_callback]\n",
    "                              )\n",
    "        model_best = tf.keras.models.load_model(checkpoint_filepath)\n",
    "        fold+=1\n",
    "        loss_T = model_best.evaluate([bert_train_input[0][train_index],bert_train_input[1][train_index],bert_train_input[2][train_index]]\n",
    "                                            , bert_train_output[train_index], verbose=0)\n",
    "        loss_V = model_best.evaluate([bert_train_input[0][test_index],bert_train_input[1][test_index],bert_train_input[2][test_index]]\n",
    "                                          , bert_train_output[test_index], verbose=0)\n",
    "        loss_test = model_best.evaluate([bert_test_input[0],bert_test_input[1],bert_test_input[2]]\n",
    "                                      , bert_test_output, verbose=0)\n",
    "        \n",
    "        train_loss.append(loss_T)\n",
    "        vald_loss.append(loss_V)\n",
    "        history.append(train_history)\n",
    "        test_loss.append(loss_test)\n",
    "            \n",
    "    test_loss_all_years.append(test_loss)\n",
    "    train_loss_all_years.append(train_loss)\n",
    "    val_loss_all_years.append(vald_loss)\n",
    "    history_all_years.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d82ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(data,columns=['year','train_df_length','test_df_length'])\n",
    "stats_df.to_csv('Loss_values/BERT_stats_minmax.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4160c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "data=[test_loss]\n",
    "  \n",
    "fig = plt.figure()  \n",
    "# Creating axes instance \n",
    "ax = fig.add_axes([0, 0, 1, 1]) \n",
    "  \n",
    "# Creating plot \n",
    "ax.boxplot(data)\n",
    "years = [year for year in range(2008,2014)]\n",
    "ax.set_xticklabels([year for year in range(2008,2014)]) \n",
    "\n",
    "# naming the y axis \n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title(\"Box plot for Test Loss\")\n",
    "textstr ='Test Loss for BERT : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
    "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
    "# show plot \n",
    "plt.savefig('Plots/block_plot_BERT_minmax.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ee417",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = []\n",
    "train_data = []\n",
    "vald_data = []\n",
    "for year_loss_test,year_loss_train,year_loss_vald,year in zip(test_loss_all_years,train_loss_all_years,val_loss_all_years,years) :\n",
    "    loss_data.append({'year':year,'value':year_loss_test})\n",
    "    train_data.append({'year':year,'value':year_loss_train})\n",
    "    vald_data.append({'year':year,'value':year_loss_vald})\n",
    "    \n",
    "loss_data_test_df = pd.DataFrame(loss_data,columns=['year','value'])\n",
    "loss_data_test_df.to_csv('Loss_values/BERT_Loss_test_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_train_df = pd.DataFrame(train_data,columns=['year','value'])\n",
    "loss_data_train_df.to_csv('Loss_values/BERT_Loss_train_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_vald_df = pd.DataFrame(vald_data,columns=['year','value'])\n",
    "loss_data_vald_df.to_csv('Loss_values/BERT_Loss_vald_minmax.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
