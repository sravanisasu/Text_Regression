{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0f1fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Required imports\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import os as os\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AlbertTokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras import optimizers\n",
    "from keras.metrics import MeanSquaredError\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "143d7a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using C:\\Users\\catuser\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/albert_en_base/1'.\n",
      "INFO:absl:Downloaded https://tfhub.dev/tensorflow/albert_en_base/1, Total size: 48.10MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/albert_en_base/1'.\n"
     ]
    }
   ],
   "source": [
    "###### ALBERT Layer\n",
    "module_url = 'https://tfhub.dev/tensorflow/albert_en_base/1'\n",
    "albert_layer = hub.KerasLayer(module_url, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ac4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to extract the input text from the files ########\n",
    "def process_inp_doc(path_file) :\n",
    "\n",
    "  file_text = open(path_file,encoding='utf8').read()\n",
    "\n",
    "  # remove punctations and digits and remove <PAGE> which was used for page number\n",
    "  file_data = re.sub(r'[\\d$%-:;!]', '', file_text)\n",
    "  file_data = re.sub(r'<PAGE>', '', file_data)\n",
    "  file_data = ''.join(file_data)\n",
    "\n",
    "  return file_data\n",
    "\n",
    "######## Function to extract the output values from the file ########\n",
    "def process_out(company_id,output_file):\n",
    "  \n",
    "  with open(output_file,'r', encoding='utf-8') as m_file :\n",
    "    for line in m_file.readlines():\n",
    "      if company_id == line.split()[1]:\n",
    "        return line.split()[0]\n",
    "    print(\"not found\")\n",
    "  return None\n",
    "\n",
    "######## Function to pre-process the documents from meta-file of a given year ########\n",
    "def pre_processing(meta_file,output_file):\n",
    "  \n",
    "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
    "    \n",
    "    year = meta_file.split('/')[1].split('.')[0]\n",
    "    dir_path = os.path.dirname(meta_file) + '/' +year+'.tok'\n",
    "    data =[]\n",
    "    for line in m_file.readlines():\n",
    "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
    "\n",
    "      # get input sentences from the company document\n",
    "      inp_sentences = process_inp_doc(inp_path_file)\n",
    "    \n",
    "      # get output value for the company\n",
    "      out_values = float(process_out(line.split()[0],output_file))\n",
    "\n",
    "      #insert values into the data list\n",
    "      data.append({'text':inp_sentences,'value':out_values})\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e3a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to get the encoded values ######## \n",
    "def albert_encode(sentences, tokenizer, MAX_SEQ_LEN=512):\n",
    "\n",
    "  all_tokens = []\n",
    "  all_masks = []\n",
    "  all_segments = []\n",
    "  for sentence in sentences:\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = stokens[-MAX_SEQ_LEN+2:]\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(stokens,)\n",
    "\n",
    "    ids = token_ids + [0] * (MAX_SEQ_LEN-len(token_ids))\n",
    "    masks = [1]*len(token_ids) + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    segments = [0] * (MAX_SEQ_LEN)\n",
    "\n",
    "    all_tokens.append(ids)\n",
    "    all_masks.append(masks)\n",
    "    all_segments.append(segments)\n",
    "\n",
    "  return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61297b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### function that defines the model\n",
    "def get_model():\n",
    "\n",
    "  input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_word_ids\")\n",
    "  input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_mask\")\n",
    "  segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"segment_ids\")\n",
    "\n",
    "  custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
    "\n",
    "  pooled_output, sequence_output = albert_layer([input_word_ids, input_mask, segment_ids])\n",
    "  clf_output = pooled_output\n",
    "  \n",
    "  net = tf.keras.layers.Dense(64, activation=custom_objects['leaky_relu'])(clf_output)\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = Flatten()(net)\n",
    "  out = tf.keras.layers.Dense(1, activation=custom_objects['leaky_relu'])(net)\n",
    "\n",
    "  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "\n",
    "  opt = optimizers.Adam(learning_rate=0.05)\n",
    "  model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8295cdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 11683584    input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           49216       keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            65          flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,732,865\n",
      "Trainable params: 49,281\n",
      "Non-trainable params: 11,683,584\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#### model summary \n",
    "MAX_SEQ_LEN = 512\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8106fb7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '10k-sample/all.meta/2005.meta.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6270cba05836>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m######## extracting text and storing it in dataframes ########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#     with tf.device('/device:GPU:0'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-80ec9cdf9c22>\u001b[0m in \u001b[0;36mpre_processing\u001b[1;34m(meta_file, output_file)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mm_file\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0myear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '10k-sample/all.meta/2005.meta.txt'"
     ]
    }
   ],
   "source": [
    "##### Data extraction and Fitting the model\n",
    "test_loss_all_years = []\n",
    "train_loss_all_years = []\n",
    "val_loss_all_years = []\n",
    "history_all_years = []\n",
    "data = []\n",
    "n_splits = 5\n",
    "epochs = 5\n",
    "for year in range(2008,2014):\n",
    "    \n",
    "    ######## extracting text and storing it in dataframes ########\n",
    "#     with tf.device('/device:GPU:0'):\n",
    "    data_train = pre_processing('10k-sample/all.meta/'+str(year-3)+'.meta.txt','10k-sample/all.logfama/'+str(year-3)+'.logfama.txt')\n",
    "    data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-2)+'.meta.txt','10k-sample/all.logfama/'+str(year-2)+'.logfama.txt'))\n",
    "    data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-1)+'.meta.txt','10k-sample/all.logfama/'+str(year-1)+'.logfama.txt'))\n",
    "    train_df = pd.DataFrame(data_train,columns=['token','value'])\n",
    "\n",
    "    data_test = pre_processing('10k-sample/all.meta/'+str(year)+'.meta.txt','10k-sample/all.logfama/'+str(year)+'.logfama.txt')\n",
    "    test_df = pd.DataFrame(data_test,columns=['token','value'])\n",
    "    data.append({'year':year,'train_df_length':len(data_train),'test_df_length':len(data_test)})\n",
    "    \n",
    "    ###### removing few documents which are not processed properly####\n",
    "    train_df = train_df.loc[train_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "    test_df = test_df.loc[test_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "    \n",
    "    ######## extracting tokens from dataframes ########\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v1\")\n",
    "    \n",
    "    # with tf.device('/device:GPU:0'):\n",
    "    \n",
    "    #### training \n",
    "    # input encoding\n",
    "    sentences = train_df.text.values\n",
    "    albert_train_input = albert_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "    # output values\n",
    "    albert_train_output = train_df.value.values\n",
    "\n",
    "    #### test\n",
    "    # input encoding\n",
    "    sentences = test_df.text.values\n",
    "    albert_test_input = albert_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "    # output values\n",
    "    albert_test_output = test_df.value.values\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    albert_train_output = np.array(albert_train_output).reshape(len(albert_train_output),1)\n",
    "    albert_test_output = np.array(albert_test_output).reshape(len(albert_test_output),1)\n",
    "    output = np.concatenate((albert_train_output, albert_test_output))\n",
    "    output = scaler.fit_transform(output)\n",
    "    albert_train_output = output[:len(albert_train_input[0])]\n",
    "    albert_test_output = output[-len(albert_test_input[0]):]\n",
    "    \n",
    "     ######## Kfold training and saving checkpoints ########\n",
    "#     with tf.device('/device:GPU:0'):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    history =[]\n",
    "    train_loss=[]\n",
    "    vald_loss=[]\n",
    "    test_loss = []\n",
    "    fold = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(albert_train_input):\n",
    "\n",
    "        checkpoint_filepath = 'Results/ALBERT_results_min/CheckPoints/'+str(year)+'ALBERT_checkpoint'+str(fold)\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                                        filepath=checkpoint_filepath,\n",
    "                                                                        save_weights_only=False,\n",
    "                                                                        monitor='loss',\n",
    "                                                                        mode='min',\n",
    "                                                                        save_best_only=True\n",
    "                                                                        )\n",
    "\n",
    "        train_history = model.fit(\n",
    "                                  [albert_train_input[0][train_index],albert_train_input[1][train_index],albert_train_input[2][train_index]],#input\n",
    "                                  albert_train_output[train_index],#output\n",
    "                                  epochs=epochs, #epochs\n",
    "                                  verbose=1,\n",
    "                                  callbacks=[model_checkpoint_callback]\n",
    "                              )\n",
    "        model_best = tf.keras.models.load_model(checkpoint_filepath)\n",
    "        fold+=1\n",
    "        loss_T = model_best.evaluate([albert_train_input[0][train_index],albert_train_input[1][train_index],albert_train_input[2][train_index]]\n",
    "                                           , albert_train_output[train_index], verbose=0)\n",
    "        loss_V = model_best.evaluate([albert_train_input[0][test_index],albert_train_input[1][test_index],albert_train_input[2][test_index]]\n",
    "                                          , albert_train_output[test_index], verbose=0)\n",
    "        loss_test = model_best.evaluate([albert_test_input[0],albert_test_input[1],albert_test_input[2]]\n",
    "                                          , albert_test_output, verbose=0)\n",
    "        \n",
    "        train_loss.append(loss_T)\n",
    "        vald_loss.append(loss_V)\n",
    "        history.append(train_history)\n",
    "        test_loss.append(loss_test)\n",
    "            \n",
    "    test_loss_all_years.append(test_loss)\n",
    "    train_loss_all_years.append(train_loss)\n",
    "    val_loss_all_years.append(vald_loss)\n",
    "    history_all_years.append(history)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e16cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(data,columns=['year','train_df_length','test_df_length'])\n",
    "stats_df.to_csv('Loss_values/ALBERT_stats_minmax.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "data=[test_loss]\n",
    "  \n",
    "fig = plt.figure()  \n",
    "# Creating axes instance \n",
    "ax = fig.add_axes([0, 0, 1, 1]) \n",
    "  \n",
    "# Creating plot \n",
    "ax.boxplot(data)\n",
    "years = [year for year in range(2008,2014)]\n",
    "ax.set_xticklabels([year for year in range(2008,2014)]) \n",
    "\n",
    "# naming the y axis \n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title(\"Box plot for Test Loss\")\n",
    "textstr ='Test Loss for ALBERT : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
    "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
    "# show plot \n",
    "plt.savefig('Plots/block_plot_ALBERT_minmax.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8616b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = []\n",
    "train_data = []\n",
    "vald_data = []\n",
    "for year_loss_test,year_loss_train,year_loss_vald,year in zip(test_loss_all_years,train_loss_all_years,val_loss_all_years,years) :\n",
    "    loss_data.append({'year':year,'value':year_loss_test})\n",
    "    train_data.append({'year':year,'value':year_loss_train})\n",
    "    vald_data.append({'year':year,'value':year_loss_vald})\n",
    "    \n",
    "loss_data_test_df = pd.DataFrame(loss_data,columns=['year','value'])\n",
    "loss_data_test_df.to_csv('Loss_values/ALBERT_Loss_test_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_train_df = pd.DataFrame(train_data,columns=['year','value'])\n",
    "loss_data_train_df.to_csv('Loss_values/ALBERT_Loss_train_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_vald_df = pd.DataFrame(vald_data,columns=['year','value'])\n",
    "loss_data_vald_df.to_csv('Loss_values/ALBERT_Loss_vald_minmax.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
