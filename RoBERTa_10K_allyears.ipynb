{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05074e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Required imports\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import os as os\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.metrics import MeanSquaredError\n",
    "from transformers import RobertaConfig\n",
    "from transformers import TFRobertaModel\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import RobertaTokenizer, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b04b5914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "###### RoBERTa Layer\n",
    "config = RobertaConfig.from_pretrained('roberta-base')\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-base',config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef5deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to extract the input text from the files ########\n",
    "def process_inp_doc(path_file) :\n",
    "\n",
    "  file_text = open(path_file,encoding='utf8').read()\n",
    "\n",
    "  # remove punctations and digits and remove <PAGE> which was used for page number\n",
    "  file_data = re.sub(r'[\\d$%-:;!]', '', file_text)\n",
    "  file_data = re.sub(r'<PAGE>', '', file_data)\n",
    "  file_data = ''.join(file_data)\n",
    "\n",
    "  return file_data\n",
    "\n",
    "######## Function to extract the output values from the file ########\n",
    "def process_out(company_id,output_file):\n",
    "  \n",
    "  with open(output_file,'r', encoding='utf-8') as m_file :\n",
    "    for line in m_file.readlines():\n",
    "      if company_id == line.split()[1]:\n",
    "        return line.split()[0]\n",
    "    print(\"not found\")\n",
    "  return None\n",
    "\n",
    "######## Function to pre-process the documents from meta-file of a given year ########\n",
    "def pre_processing(meta_file,output_file):\n",
    "  \n",
    "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
    "    \n",
    "    year = meta_file.split('/')[3].split('.')[0]\n",
    "    dir_path = os.path.dirname(meta_file) + '/' +year+'.tok'\n",
    "    data =[]\n",
    "    \n",
    "    for line in m_file.readlines():\n",
    "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
    "\n",
    "      # get input sentences from the company document\n",
    "      inp_sentences = process_inp_doc(inp_path_file)\n",
    "    \n",
    "      # get output value for the company\n",
    "      out_values = float(process_out(line.split()[0],output_file))\n",
    "\n",
    "      #insert values into the data list\n",
    "      data.append({'text':inp_sentences,'value':out_values})\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd3d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to get the encoded values ######## \n",
    "def roberta_encode(sentences, tokenizer, MAX_SEQ_LEN=512):\n",
    "\n",
    "  all_tokens = []\n",
    "  all_masks = []\n",
    "  all_segments = []\n",
    "  for sentence in sentences:\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = stokens[-MAX_SEQ_LEN+2:]\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(stokens,)\n",
    "\n",
    "    ids = token_ids + [0] * (MAX_SEQ_LEN-len(token_ids))\n",
    "    masks = [1]*len(token_ids) + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    segments = [0] * (MAX_SEQ_LEN)\n",
    "\n",
    "    all_tokens.append(ids)\n",
    "    all_masks.append(masks)\n",
    "    all_segments.append(segments)\n",
    "\n",
    "  return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e3c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### function that defines the model\n",
    "def get_model():\n",
    "\n",
    "  input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_word_ids\")\n",
    "  input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_mask\")\n",
    "  segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"segment_ids\")\n",
    "\n",
    "  model_roberta = roberta_model(input_word_ids, attention_mask  = input_mask, token_type_ids  = segment_ids)\n",
    "  clf_output = model_roberta.pooler_output  \n",
    "  \n",
    "  custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
    "\n",
    "  net = tf.keras.layers.Dropout(0.5)(clf_output)\n",
    "  net = tf.keras.layers.Dense(32, activation=custom_objects['leaky_relu'])(net)\n",
    "  net = tf.keras.layers.LayerNormalization()(net)\n",
    "  out = tf.keras.layers.Dense(1, activation=custom_objects['leaky_relu'], name='output')(net)\n",
    "\n",
    "  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "\n",
    "  opt = optimizers.Adam(learning_rate=0.05)\n",
    "  model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ec826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000000B53119EDC0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000000B53119EDC0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 124645632   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 768)          0           tf_roberta_model[0][1]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           24608       dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 32)           64          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            33          layer_normalization[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 124,670,337\n",
      "Trainable params: 124,670,337\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#### model summary \n",
    "MAX_SEQ_LEN = 512\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8e2a1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '10k-sample/all.meta/2005.meta.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9dcf9367e8e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m######## extracting text and storing it in dataframes ########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#     with tf.device('/device:GPU:0'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10k-sample/all.meta/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10k-sample/all.logfama/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.logfama.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-73be3d52e816>\u001b[0m in \u001b[0;36mpre_processing\u001b[1;34m(meta_file, output_file)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mm_file\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0myear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '10k-sample/all.meta/2005.meta.txt'"
     ]
    }
   ],
   "source": [
    "##### Data extraction and Fitting the model\n",
    "test_loss_all_years = []\n",
    "train_loss_all_years = []\n",
    "val_loss_all_years = []\n",
    "history_all_years = []\n",
    "data = []\n",
    "n_splits = 5\n",
    "epochs = 5\n",
    "for year in range(2008,2014):\n",
    "    \n",
    "    ######## extracting text and storing it in dataframes ########\n",
    "#     with tf.device('/device:GPU:0'):\n",
    "    data_train = pre_processing('10k-sample/all.meta/'+str(year-3)+'.meta.txt','10k-sample/all.logfama/'+str(year-3)+'.logfama.txt')\n",
    "    data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-2)+'.meta.txt','10k-sample/all.logfama/'+str(year-2)+'.logfama.txt'))\n",
    "    data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-1)+'.meta.txt','10k-sample/all.logfama/'+str(year-1)+'.logfama.txt'))\n",
    "    train_df = pd.DataFrame(data_train,columns=['token','value'])\n",
    "\n",
    "    data_test = pre_processing('10k-sample/all.meta/'+str(year)+'.meta.txt','10k-sample/all.logfama/'+str(year)+'.logfama.txt')\n",
    "    test_df = pd.DataFrame(data_test,columns=['token','value'])\n",
    "    data.append({'year':year,'train_df_length':len(data_train),'test_df_length':len(data_test)})\n",
    "    \n",
    "    ###### removing few documents which are not processed properly####\n",
    "    train_df = train_df.loc[train_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "    test_df = test_df.loc[test_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "    \n",
    "    ######## extracting tokens from dataframes ########\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# with tf.device('/device:GPU:0'):\n",
    "\n",
    "    #### training \n",
    "    # input encoding\n",
    "    sentences = train_df.text.values\n",
    "    roberta_train_input = roberta_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "    # output values\n",
    "    roberta_train_output = train_df.value.values\n",
    "\n",
    "    #### test\n",
    "    # input encoding\n",
    "    sentences = test_df.text.values\n",
    "    roberta_test_input = roberta_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "    # output values\n",
    "    roberta_test_output = test_df.value.values\n",
    "    \n",
    "    ### applying minmax scalar\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    roberta_train_output = np.array(roberta_train_output).reshape(len(roberta_train_output),1)\n",
    "    roberta_test_output = np.array(roberta_test_output).reshape(len(roberta_test_output),1)\n",
    "    output = np.concatenate((roberta_train_output, roberta_test_output))\n",
    "    output = scaler.fit_transform(output)\n",
    "    roberta_train_output = output[:len(roberta_train_input[0])]\n",
    "    roberta_test_output = output[-len(roberta_test_input[0]):]\n",
    "    \n",
    "    ######## Kfold training and saving checkpoints ########\n",
    "#     with tf.device('/device:GPU:0'):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    history =[]\n",
    "    train_loss=[]\n",
    "    vald_loss=[]\n",
    "    test_loss = []\n",
    "    fold = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(roberta_train_input):\n",
    "\n",
    "        checkpoint_filepath = 'Results/RoBERTa_results_min/CheckPoints/'+str(year)+'RoBERTa_checkpoint'+str(fold)\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                                        filepath=checkpoint_filepath,\n",
    "                                                                        save_weights_only=True,\n",
    "                                                                        monitor='loss',\n",
    "                                                                        mode='min',\n",
    "                                                                        save_best_only=True\n",
    "                                                                    )\n",
    "\n",
    "        train_history = model.fit(\n",
    "                                  [roberta_train_input[0][train_index],roberta_train_input[1][train_index],roberta_train_input[2][train_index]],#input\n",
    "                                  roberta_train_output[train_index],#output\n",
    "                                  epochs=epochs, #epochs\n",
    "                                  verbose=1,\n",
    "                                  batch_size = batch_size,\n",
    "                                  callbacks=[model_checkpoint_callback]\n",
    "                              )\n",
    "        model_best = get_model()\n",
    "        model_best.load_weights(checkpoint_filepath)\n",
    "\n",
    "        fold+=1\n",
    "        loss_T = model_best.evaluate([roberta_train_input[0][train_index],roberta_train_input[1][train_index],roberta_train_input[2][train_index]]\n",
    "                                           , roberta_train_output[train_index], verbose=0)\n",
    "        loss_V = model_best.evaluate([roberta_train_input[0][test_index],roberta_train_input[1][test_index],roberta_train_input[2][test_index]]\n",
    "                                          , roberta_train_output[test_index], verbose=0)\n",
    "        loss_test = model_best.evaluate([roberta_test_input[0],roberta_test_input[1],roberta_test_input[2]]\n",
    "                                          , roberta_test_output, verbose=0)\n",
    "        \n",
    "        train_loss.append(loss_T)\n",
    "        vald_loss.append(loss_V)\n",
    "        history.append(train_history)\n",
    "        test_loss.append(loss_test)\n",
    "            \n",
    "    test_loss_all_years.append(test_loss)\n",
    "    train_loss_all_years.append(train_loss)\n",
    "    val_loss_all_years.append(vald_loss)\n",
    "    history_all_years.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(data,columns=['year','train_df_length','test_df_length'])\n",
    "stats_df.to_csv('Loss_values/BERT_stats_minmax.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "data=[test_loss]\n",
    "  \n",
    "fig = plt.figure()  \n",
    "# Creating axes instance \n",
    "ax = fig.add_axes([0, 0, 1, 1]) \n",
    "  \n",
    "# Creating plot \n",
    "ax.boxplot(data)\n",
    "years = [year for year in range(2008,2014)]\n",
    "ax.set_xticklabels([year for year in range(2008,2014)]) \n",
    "\n",
    "# naming the y axis \n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title(\"Box plot for Test Loss\")\n",
    "textstr ='Test Loss for RoBERTa : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
    "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
    "# show plot \n",
    "plt.savefig('Plots/block_plot_RoBERTa_minmax.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4813939",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = []\n",
    "train_data = []\n",
    "vald_data = []\n",
    "for year_loss_test,year_loss_train,year_loss_vald,year in zip(test_loss_all_years,train_loss_all_years,val_loss_all_years,years) :\n",
    "    loss_data.append({'year':year,'value':year_loss_test})\n",
    "    train_data.append({'year':year,'value':year_loss_train})\n",
    "    vald_data.append({'year':year,'value':year_loss_vald})\n",
    "    \n",
    "loss_data_test_df = pd.DataFrame(loss_data,columns=['year','value'])\n",
    "loss_data_test_df.to_csv('Loss_values/RoBERTa_Loss_test_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_train_df = pd.DataFrame(train_data,columns=['year','value'])\n",
    "loss_data_train_df.to_csv('Loss_values/RoBERTa_Loss_train_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_vald_df = pd.DataFrame(vald_data,columns=['year','value'])\n",
    "loss_data_vald_df.to_csv('Loss_values/RoBERTa_Loss_vald_minmax.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
