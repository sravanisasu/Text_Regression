{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8f92ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\catuser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing the required packages\n",
    "\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building embedding for the words\n",
    "embed_file = \"10k-sample/sim.expand.200d.vec\"\n",
    "\n",
    "#Define Hyper parameters\n",
    "max_inp_len = 20000\n",
    "# the dimension of vectors to be used\n",
    "embed_dim = 200\n",
    "rounding = 6\n",
    "# filter sizes of the different conv layers \n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 1\n",
    "pool_size = 199\n",
    "# dropout probability\n",
    "drop = 0.5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832eacdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define embedding dictionary and embed matrix for the vocabulary\n",
    "embeddings_dic = dict()\n",
    "f = open(embed_file,encoding='utf8')\n",
    "with open(embed_file, 'r', encoding='utf-8') as e_file:\n",
    "  for line in e_file:\n",
    "    splitlines = line.split()\n",
    "    word = splitlines[0].strip()\n",
    "    coefs = np.asarray(splitlines[1:], dtype='float32')\n",
    "    embeddings_dic[word] = coefs\n",
    "\n",
    "print(\"length of embedding dictionary\",len(embeddings_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(embeddings_dic.keys())\n",
    "embed_token = Tokenizer()\n",
    "embed_token.fit_on_texts(embeddings_dic.keys())\n",
    "embedding_matrix = np.zeros((vocabulary_size, embed_dim))\n",
    "for word, index in embed_token.word_index.items():\n",
    "    embedding_matrix[index] = embeddings_dic.get(word)\n",
    "print(\"embedding_matrix dimension\",len(embedding_matrix),len(embedding_matrix[0]))\n",
    "print(\"no of token in the tokenizer\",len(embed_token.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pre process the document\n",
    "def process_doc(path_file,embed_token) :\n",
    "\n",
    "  #tokenizing the words \n",
    "  with open(path_file,'r', encoding='utf-8') as tok_file :\n",
    "    file_words = list(tok_file)[0].split()\n",
    "    \n",
    "  #removing the stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  filtered_words = []  \n",
    "  for word in file_words: \n",
    "      if word not in stop_words and word.isalpha(): \n",
    "          filtered_words.append(word)\n",
    "\n",
    "  # applying stemming using PorterStemmer\n",
    "\n",
    "  p_stemmer = PorterStemmer()\n",
    "  stem_words=[]\n",
    "  for word in filtered_words:\n",
    "    stem_words.append(p_stemmer.stem(word))\n",
    "    \n",
    "  #tokenizing the words using the embed token\n",
    "  tokens=[]\n",
    "  for word in stem_words:\n",
    "    try:\n",
    "      tokens.append(embed_token.word_index[word])\n",
    "    except:\n",
    "      tokens.append(1)\n",
    "\n",
    "  if len(tokens) < max_inp_len:\n",
    "    tokens.extend([0]*(max_inp_len-len(tokens)))\n",
    "  else:\n",
    "    tokens = tokens[:max_inp_len]\n",
    "    \n",
    "  return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output dataset\n",
    "def output_data(company_id, out_path_file):\n",
    "  with open(out_path_file,'r', encoding='utf-8') as out_file :\n",
    "    for line in out_file.readlines():\n",
    "      if company_id == line.split()[1]:\n",
    "        return line.split()[0]\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b435de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(meta_file,output_file):\n",
    "\n",
    "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
    "    \n",
    "    year = meta_file.split('/')[2].split('.')[0]\n",
    "    dir_path = os.path.dirname(meta_file).split('/')[0] + '/all.tok/' +year+'.tok'\n",
    "    data =[]\n",
    "\n",
    "    for line in m_file.readlines():\n",
    "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
    "      \n",
    "      # get input tokens from the company document\n",
    "      inp_tokens = process_doc(inp_path_file,embed_token)\n",
    "      \n",
    "      # get output value for the company\n",
    "      out_values = output_data(line.split()[0],output_file)\n",
    "\n",
    "      #insert values into the data list\n",
    "      data.append({'token':inp_tokens,'value':out_values})\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec87207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(max_inp_len,vocabulary_size,embed_dim,filter_sizes,num_filters,pool_size,drop,learning_rate):\n",
    "  \n",
    "  # input and embedding matrix\n",
    "  inputs = Input(shape=(max_inp_len,))\n",
    "  embedding = Embedding(vocabulary_size, embed_dim, weights=[embedding_matrix],trainable = False)(inputs)\n",
    "\n",
    "  custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
    "\n",
    " # Layer1 RNN with LSTM\n",
    "  layer_1 = LSTM(units=64, activation = 'tanh' )(embedding)\n",
    "       \n",
    "  # 1 fully connected layers\n",
    "  outputs = Dense(1, activation=custom_objects['leaky_relu'])(layer_1)\n",
    "\n",
    "  model = Model(inputs=[inputs], outputs=outputs)\n",
    "    \n",
    "  opt = optimizers.SGD(learning_rate=learning_rate)\n",
    "  model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "model = define_model(max_inp_len,vocabulary_size,embed_dim,filter_sizes,num_filters,pool_size,drop,learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ba619",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_all_years = []\n",
    "train_loss_all_years = []\n",
    "val_loss_all_years = []\n",
    "history_all_years = []\n",
    "data = []\n",
    "n_splits = 5\n",
    "for year in range(2008,2014):\n",
    "    \n",
    "    ######## extracting text and storing it in dataframes ########      \n",
    "    data_train = pre_processing('10k-sample/all.meta/'+str(year-3)+'.meta.txt','10k-sample/all.logfama/'+str(year-3)+'.logfama.txt')\n",
    "    data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-2)+'.meta.txt','10k-sample/all.logfama/'+str(year-2)+'.logfama.txt'))\n",
    "    data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-1)+'.meta.txt','10k-sample/all.logfama/'+str(year-1)+'.logfama.txt'))\n",
    "    train_df = pd.DataFrame(data_train,columns=['token','value'])\n",
    "\n",
    "    data_test = pre_processing('10k-sample/all.meta/'+str(year)+'.meta.txt','10k-sample/all.logfama/'+str(year)+'.logfama.txt')\n",
    "    test_df = pd.DataFrame(data_test,columns=['token','value'])\n",
    "    data.append({'year':year,'train_df_length':len(data_train),'test_df_length':len(data_test)})\n",
    "\n",
    "    ######## reshapping data to required format ########\n",
    "    RNN_train_input = train_df.token.values\n",
    "    RNN_train_output = [ float(x) for x in train_df.value.values ]\n",
    "    RNN_test_input = test_df.token.values\n",
    "    RNN_test_output = [ float(x) for x in test_df.value.values ]\n",
    "    RNN_train_output = np.array(RNN_train_output).reshape(len(RNN_train_output),1)\n",
    "    RNN_test_output = np.array(RNN_test_output).reshape(len(RNN_test_output),1)\n",
    "\n",
    "    RNN_train_input = np.stack(RNN_train_input)\n",
    "    RNN_test_input = np.stack(RNN_test_input)\n",
    "    \n",
    "    ### applying minmax scalar\n",
    "    scaler = MinMaxScaler()\n",
    "    RNN_train_output = np.array(RNN_train_output).reshape(len(RNN_train_output),1)\n",
    "    RNN_test_output = np.array(RNN_test_output).reshape(len(RNN_test_output),1)\n",
    "    output = np.concatenate((RNN_train_output, RNN_test_output))\n",
    "    output = scaler.fit_transform(output)\n",
    "    RNN_train_output = output[:len(RNN_train_input)]\n",
    "    RNN_test_output = output[-len(RNN_test_input):]\n",
    "    \n",
    "    ######## Kfold training and saving checkpoints ########\n",
    "# with tf.device('/device:GPU:0'):\n",
    "\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    history =[]\n",
    "    train_loss=[]\n",
    "    vald_loss=[]\n",
    "    test_loss = []\n",
    "    fold = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(RNN_train_input):\n",
    "\n",
    "        checkpoint_filepath = 'Results/RNN_results_min/CheckPoints/'+str(year)+'RNN_checkpoint'+str(fold)\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                                        filepath=checkpoint_filepath,\n",
    "                                                                        save_weights_only=False,\n",
    "                                                                        monitor='loss',\n",
    "                                                                        mode='min',\n",
    "                                                                        save_best_only=True\n",
    "                                                                        )\n",
    "        train_history = model.fit(\n",
    "                                  RNN_train_input[train_index],\n",
    "                                  RNN_train_output[train_index],#output\n",
    "                                  epochs=epochs, #epochs\n",
    "                                  verbose=1,\n",
    "                                  callbacks=[model_checkpoint_callback]\n",
    "                              )\n",
    "        model_best = tf.keras.models.load_model(checkpoint_filepath)\n",
    "        fold+=1\n",
    "        loss_T = model_best.evaluate(RNN_train_input[train_index],RNN_train_output[train_index], verbose=0)\n",
    "        loss_V = model_best.evaluate(RNN_train_input[test_index],RNN_train_output[test_index], verbose=0)\n",
    "        loss_test = model_best.evaluate(RNN_test_input,RNN_test_output, verbose=0)\n",
    "        \n",
    "        train_loss.append(loss_T)\n",
    "        vald_loss.append(loss_V)\n",
    "        history.append(train_history)\n",
    "        test_loss.append(loss_test)\n",
    "\n",
    "    test_loss_all_years.append(test_loss)\n",
    "    train_loss_all_years.append(train_loss)\n",
    "    val_loss_all_years.append(vald_loss)\n",
    "    history_all_years.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c99828",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(data,columns=['year','train_df_length','test_df_length'])\n",
    "stats_df.to_csv('Loss_values/RNN_stats_minmax.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "data=[test_loss]\n",
    "  \n",
    "fig = plt.figure()  \n",
    "# Creating axes instance \n",
    "ax = fig.add_axes([0, 0, 1, 1]) \n",
    "  \n",
    "# Creating plot \n",
    "ax.boxplot(data)\n",
    "years = [year for year in range(2008,2014)]\n",
    "ax.set_xticklabels([year for year in range(2008,2014)]) \n",
    "\n",
    "# naming the y axis \n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title(\"Box plot for Test Loss\")\n",
    "textstr ='Test Loss for RNN : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
    "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
    "# show plot \n",
    "plt.savefig('Plots/block_plot_RNN_minmax.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0636fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = []\n",
    "train_data = []\n",
    "vald_data = []\n",
    "for year_loss_test,year_loss_train,year_loss_vald,year in zip(test_loss_all_years,train_loss_all_years,val_loss_all_years,years) :\n",
    "    loss_data.append({'year':year,'value':year_loss_test})\n",
    "    train_data.append({'year':year,'value':year_loss_train})\n",
    "    vald_data.append({'year':year,'value':year_loss_vald})\n",
    "    \n",
    "loss_data_test_df = pd.DataFrame(loss_data,columns=['year','value'])\n",
    "loss_data_test_df.to_csv('Loss_values/RNN_Loss_test_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_train_df = pd.DataFrame(train_data,columns=['year','value'])\n",
    "loss_data_train_df.to_csv('Loss_values/RNN_Loss_train_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_vald_df = pd.DataFrame(vald_data,columns=['year','value'])\n",
    "loss_data_vald_df.to_csv('Loss_values/RNN_Loss_vald_minmax.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
